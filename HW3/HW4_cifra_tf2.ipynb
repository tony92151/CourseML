{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "#train, test= tf.keras.datasets.cifar10.load_data()\n",
    "# y_train = np.squeeze(y_train,axis=1)\n",
    "# y_test = np.squeeze(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(length, num):\n",
    "    d = np.zeros(length)\n",
    "    d[num] = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = np.array([onehot(10,i) for i in y_train])\n",
    "# y_test = np.array([onehot(10,i) for i in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (x_train-127.5)/127.5\n",
    "x_test = (x_test-127.5)/127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = np.array([[x_train[i],y_train[i][0]] for i in range(len(x_train))])\n",
    "# for fold_num, (trn_idx, val_idx) in enumerate(fold.split(train)):\n",
    "#     Train_data = train[trn_idx]\n",
    "#     Train_data_x = np.array([Train_data[i][0] for i in range(len(Train_data))])\n",
    "#     Train_data_y = np.array([Train_data[i][1] for i in range(len(Train_data))])\n",
    "    \n",
    "#     Val_data = train[val_idx]\n",
    "#     Val_data_x = np.array([Val_data[i][0] for i in range(len(Val_data))])\n",
    "#     Val_data_y = np.array([Val_data[i][1] for i in range(len(Val_data))])\n",
    "    \n",
    "#     train_ds = tf.data.Dataset.from_tensor_slices((Train_data_x, Train_data_y))\n",
    "#     train_ds = train_ds.shuffle(10000).batch(64)\n",
    "    \n",
    "#     val_ds = tf.data.Dataset.from_tensor_slices((Val_data_x, Val_data_y))\n",
    "#     val_ds = train_ds.shuffle(10000).batch(64)\n",
    "    \n",
    "#     print(fold_num)\n",
    "#     print(Train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(10000).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 32, 32, 3), (None, 1)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = test_ds.shuffle(10000).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4feafd4d30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[1].astype('int16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Conv2D(64,3, activation='relu',strides = 1,input_shape = (32,32,3)),\n",
    "#     tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=1, padding ='same', data_format='channels_last'),\n",
    "#     tf.keras.layers.Flatten(input_shape=(32,32.3)),##固定輸入\n",
    "#     tf.keras.layers.Dense(128, activation='relu'),\n",
    "#     tf.keras.layers.Dense(10, activation='softmax')    \n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3,activation='relu')\n",
    "        self.conv22 = Conv2D(64, 5,activation='relu',padding=\"valid\")\n",
    "        self.maxpool = MaxPooling2D()\n",
    "        self.drop1  = Dropout(rate = 0.3)\n",
    "        self.conv2 = Conv2D(64, 3,activation='tanh')\n",
    "        self.drop2  = Dropout(rate = 0.5)\n",
    "        self.maxpool2 = MaxPooling2D()\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(2048, activation='relu')\n",
    "        self.d2 = Dense(512, activation='relu')\n",
    "        self.d3 = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x_o): \n",
    "        #print(\"1: \",x.shape)\n",
    "        x = self.conv1(x_o)\n",
    "        x = self.conv2(x)\n",
    "        x = self.drop1(x)\n",
    "        \n",
    "        y = self.conv22(x_o)# 28*28*32\n",
    "        y = self.maxpool(y)# 14*14*32\n",
    "        #print(\"2: \",x.shape)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.drop1(x)# 14*14*32\n",
    "        #print(\"3: \",x.shape)\n",
    "        x = tf.concat([x, y], 3)\n",
    "        \n",
    "        x = self.drop2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        #print(\"4: \",x.shape)\n",
    "        x = self.flatten(x)\n",
    "        #print(\"5: \",x.shape)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        #print(\"6: \",x.shape)\n",
    "        return self.d3(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(logits, labels, gamma):\n",
    "    '''\n",
    "    :param logits:  [batch_size, n_class]\n",
    "    :param labels: [batch_size]\n",
    "    :return: -(1-y)^r * log(y)\n",
    "    '''\n",
    "    labels = tf.cast(labels, tf.int32)\n",
    "    softmax = tf.reshape(tf.nn.softmax(logits), [-1])  # [batch_size * n_class]\n",
    "    labels = tf.range(0, logits.shape[0]) * logits.shape[1] + labels\n",
    "    prob = tf.gather(softmax, labels)\n",
    "    weight = tf.pow(tf.subtract(1., prob), gamma)\n",
    "    loss = -tf.reduce_mean(tf.multiply(weight, tf.log(prob)))\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_category_focal_loss1(y_true, y_pred):\n",
    "    epsilon = 1.e-7\n",
    "    gamma = 2.0\n",
    "    #alpha = tf.constant([[2],[1],[1],[1],[1]], dtype=tf.float32)\n",
    "    alpha = tf.constant([[1],[1],[1],[1],[1],[1],[1],[1],[1],[1]], dtype=tf.float32)\n",
    "\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "    y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "    ce = -tf.math.log(y_t/10)\n",
    "    weight = tf.pow(tf.subtract(1., y_t), gamma)\n",
    "    #print(weight)\n",
    "    fl = tf.matmul(tf.multiply(weight, ce), alpha)\n",
    "    loss = tf.reduce_mean(fl)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, labels):\n",
    "    #print(\"I :\",images.shape)\n",
    "    #print(\"L :\",labels)\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        #print(\"P: \",predictions.shape)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        #print(loss)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    #print(\"train loss: \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    loss = loss_object(labels, predictions)\n",
    "    val_loss(loss)\n",
    "    val_accuracy(labels, predictions)\n",
    "    #print(\"val loss: \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 20\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     for images, labels in train_ds:\n",
    "#         train_step(images, labels)\n",
    "\n",
    "#     for test_images, test_labels in test_ds:\n",
    "#         test_step(test_images, test_labels)\n",
    "\n",
    "#     #template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "#     print('Epoch {}/{}, Loss: {:.5f}, Accuracy: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.\n",
    "#             format(\n",
    "#                 epoch+1,\n",
    "#                 EPOCHS,\n",
    "#                 train_loss.result(),\n",
    "#                 train_accuracy.result()*100,\n",
    "#                 test_loss.result(),\n",
    "#                 test_accuracy.result()*100))\n",
    "\n",
    "#     # Reset the metrics for the next epoch\n",
    "#     train_loss.reset_states()\n",
    "#     train_accuracy.reset_states()\n",
    "#     test_loss.reset_states()\n",
    "#     test_accuracy.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = KFold(n_splits = 5, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F 1/5, Ep 1/20, Loss: 1.28018, Val Acc: 66.97000, Test Loss: 0.94791, Test Acc: 66.98000\n",
      "F 1/5, Ep 2/20, Loss: 0.81581, Val Acc: 71.32000, Test Loss: 0.82028, Test Acc: 71.03001\n",
      "F 1/5, Ep 3/20, Loss: 0.55958, Val Acc: 74.11000, Test Loss: 0.81661, Test Acc: 73.03000\n",
      "F 1/5, Ep 4/20, Loss: 0.33353, Val Acc: 74.07000, Test Loss: 0.86996, Test Acc: 73.32000\n",
      "F 1/5, Ep 5/20, Loss: 0.17276, Val Acc: 74.24000, Test Loss: 1.13422, Test Acc: 73.53000\n",
      "F 1/5, Ep 6/20, Loss: 0.12097, Val Acc: 74.83000, Test Loss: 1.22659, Test Acc: 73.65000\n",
      "F 1/5, Ep 7/20, Loss: 0.09192, Val Acc: 74.31000, Test Loss: 1.30473, Test Acc: 73.74000\n",
      "F 1/5, Ep 8/20, Loss: 0.08449, Val Acc: 74.75000, Test Loss: 1.37619, Test Acc: 73.28000\n",
      "F 1/5, Ep 9/20, Loss: 0.07003, Val Acc: 74.23000, Test Loss: 1.45415, Test Acc: 73.50999\n",
      "F 1/5, Ep 10/20, Loss: 0.07435, Val Acc: 73.30000, Test Loss: 1.50365, Test Acc: 73.00999\n",
      "F 1/5, Ep 11/20, Loss: 0.05891, Val Acc: 74.73000, Test Loss: 1.58918, Test Acc: 73.94000\n",
      "F 1/5, Ep 12/20, Loss: 0.06250, Val Acc: 73.07000, Test Loss: 1.74179, Test Acc: 72.55000\n",
      "F 1/5, Ep 13/20, Loss: 0.05613, Val Acc: 75.20000, Test Loss: 1.69225, Test Acc: 74.05000\n",
      "F 1/5, Ep 14/20, Loss: 0.05140, Val Acc: 73.83000, Test Loss: 1.76405, Test Acc: 72.75999\n",
      "F 1/5, Ep 15/20, Loss: 0.05163, Val Acc: 73.94000, Test Loss: 1.70964, Test Acc: 73.45000\n",
      "F 1/5, Ep 16/20, Loss: 0.05395, Val Acc: 73.89000, Test Loss: 1.75462, Test Acc: 73.68999\n",
      "F 1/5, Ep 17/20, Loss: 0.04309, Val Acc: 74.11000, Test Loss: 1.80080, Test Acc: 74.09000\n",
      "F 1/5, Ep 18/20, Loss: 0.04054, Val Acc: 74.03000, Test Loss: 1.97131, Test Acc: 73.63000\n",
      "F 1/5, Ep 19/20, Loss: 0.04880, Val Acc: 74.09000, Test Loss: 1.83006, Test Acc: 73.30000\n",
      "F 1/5, Ep 20/20, Loss: 0.04512, Val Acc: 74.30000, Test Loss: 1.99418, Test Acc: 73.83000\n",
      "F 2/5, Ep 1/20, Loss: 0.30810, Val Acc: 99.00000, Test Loss: 1.01621, Test Acc: 75.58000\n",
      "F 2/5, Ep 2/20, Loss: 0.10349, Val Acc: 98.40000, Test Loss: 1.20328, Test Acc: 76.16000\n",
      "F 2/5, Ep 3/20, Loss: 0.03395, Val Acc: 97.07000, Test Loss: 1.47798, Test Acc: 74.23000\n",
      "F 2/5, Ep 4/20, Loss: 0.03250, Val Acc: 97.39000, Test Loss: 1.62359, Test Acc: 74.09000\n",
      "F 2/5, Ep 5/20, Loss: 0.05719, Val Acc: 96.38000, Test Loss: 1.64868, Test Acc: 73.85000\n",
      "F 2/5, Ep 6/20, Loss: 0.05359, Val Acc: 96.74000, Test Loss: 1.69361, Test Acc: 74.93000\n",
      "F 2/5, Ep 7/20, Loss: 0.04487, Val Acc: 94.50000, Test Loss: 1.78892, Test Acc: 72.66000\n",
      "F 2/5, Ep 8/20, Loss: 0.04878, Val Acc: 95.86000, Test Loss: 1.68204, Test Acc: 73.93000\n",
      "F 2/5, Ep 9/20, Loss: 0.02882, Val Acc: 96.32000, Test Loss: 1.74692, Test Acc: 74.84000\n",
      "F 2/5, Ep 10/20, Loss: 0.04125, Val Acc: 94.87000, Test Loss: 1.85465, Test Acc: 74.03000\n",
      "F 2/5, Ep 11/20, Loss: 0.04641, Val Acc: 94.73000, Test Loss: 1.82711, Test Acc: 73.61000\n",
      "F 2/5, Ep 12/20, Loss: 0.03646, Val Acc: 94.80000, Test Loss: 2.02743, Test Acc: 74.19000\n",
      "F 2/5, Ep 13/20, Loss: 0.04658, Val Acc: 93.61000, Test Loss: 2.03157, Test Acc: 74.05000\n",
      "F 2/5, Ep 14/20, Loss: 0.03819, Val Acc: 94.22000, Test Loss: 1.98018, Test Acc: 74.04000\n",
      "F 2/5, Ep 15/20, Loss: 0.04204, Val Acc: 93.72000, Test Loss: 1.95273, Test Acc: 74.81000\n",
      "F 2/5, Ep 16/20, Loss: 0.03307, Val Acc: 91.43000, Test Loss: 2.11379, Test Acc: 72.98000\n",
      "F 2/5, Ep 17/20, Loss: 0.03501, Val Acc: 92.84000, Test Loss: 2.07219, Test Acc: 74.29000\n",
      "F 2/5, Ep 18/20, Loss: 0.03320, Val Acc: 92.48000, Test Loss: 2.03408, Test Acc: 74.51000\n",
      "F 2/5, Ep 19/20, Loss: 0.03696, Val Acc: 91.45000, Test Loss: 1.90881, Test Acc: 73.38000\n",
      "F 2/5, Ep 20/20, Loss: 0.03740, Val Acc: 91.66000, Test Loss: 2.05713, Test Acc: 74.89000\n",
      "F 3/5, Ep 1/20, Loss: 0.12479, Val Acc: 99.32000, Test Loss: 1.60368, Test Acc: 75.09000\n",
      "F 3/5, Ep 2/20, Loss: 0.03357, Val Acc: 99.31000, Test Loss: 1.76918, Test Acc: 75.82000\n",
      "F 3/5, Ep 3/20, Loss: 0.01450, Val Acc: 99.10000, Test Loss: 1.96313, Test Acc: 75.24000\n",
      "F 3/5, Ep 4/20, Loss: 0.04001, Val Acc: 98.15000, Test Loss: 2.15683, Test Acc: 74.27000\n",
      "F 3/5, Ep 5/20, Loss: 0.04028, Val Acc: 98.43999, Test Loss: 1.98771, Test Acc: 74.50000\n",
      "F 3/5, Ep 6/20, Loss: 0.03876, Val Acc: 97.93000, Test Loss: 2.14198, Test Acc: 73.91000\n",
      "F 3/5, Ep 7/20, Loss: 0.03930, Val Acc: 97.59000, Test Loss: 2.05282, Test Acc: 73.59000\n",
      "F 3/5, Ep 8/20, Loss: 0.04118, Val Acc: 98.18000, Test Loss: 2.04433, Test Acc: 74.59000\n",
      "F 3/5, Ep 9/20, Loss: 0.03568, Val Acc: 98.00000, Test Loss: 2.01719, Test Acc: 74.90000\n",
      "F 3/5, Ep 10/20, Loss: 0.03680, Val Acc: 98.16000, Test Loss: 2.17651, Test Acc: 74.90000\n",
      "F 3/5, Ep 11/20, Loss: 0.03036, Val Acc: 97.43999, Test Loss: 2.36731, Test Acc: 74.87000\n",
      "F 3/5, Ep 12/20, Loss: 0.03509, Val Acc: 96.34000, Test Loss: 2.24870, Test Acc: 73.60000\n",
      "F 3/5, Ep 13/20, Loss: 0.04929, Val Acc: 97.46000, Test Loss: 2.21599, Test Acc: 74.35000\n",
      "F 3/5, Ep 14/20, Loss: 0.02853, Val Acc: 97.35000, Test Loss: 2.21917, Test Acc: 74.20000\n",
      "F 3/5, Ep 15/20, Loss: 0.04293, Val Acc: 96.83000, Test Loss: 2.43097, Test Acc: 74.53000\n",
      "F 3/5, Ep 16/20, Loss: 0.04091, Val Acc: 96.34000, Test Loss: 2.34428, Test Acc: 73.97000\n",
      "F 3/5, Ep 17/20, Loss: 0.03300, Val Acc: 95.59000, Test Loss: 2.41884, Test Acc: 73.36000\n",
      "F 3/5, Ep 18/20, Loss: 0.03817, Val Acc: 96.68000, Test Loss: 2.29478, Test Acc: 74.38000\n",
      "F 3/5, Ep 19/20, Loss: 0.03110, Val Acc: 96.88000, Test Loss: 2.38923, Test Acc: 75.10000\n",
      "F 3/5, Ep 20/20, Loss: 0.04061, Val Acc: 94.44000, Test Loss: 2.66778, Test Acc: 73.24000\n",
      "F 4/5, Ep 1/20, Loss: 0.08836, Val Acc: 99.38000, Test Loss: 1.98028, Test Acc: 74.16000\n",
      "F 4/5, Ep 2/20, Loss: 0.03242, Val Acc: 99.38000, Test Loss: 1.99705, Test Acc: 74.64000\n",
      "F 4/5, Ep 3/20, Loss: 0.02165, Val Acc: 99.38000, Test Loss: 2.03022, Test Acc: 75.01000\n",
      "F 4/5, Ep 4/20, Loss: 0.02636, Val Acc: 98.84000, Test Loss: 2.27948, Test Acc: 74.10000\n",
      "F 4/5, Ep 5/20, Loss: 0.04130, Val Acc: 98.77000, Test Loss: 2.41661, Test Acc: 74.73000\n",
      "F 4/5, Ep 6/20, Loss: 0.05101, Val Acc: 98.83000, Test Loss: 2.22133, Test Acc: 75.10000\n",
      "F 4/5, Ep 7/20, Loss: 0.03207, Val Acc: 99.01000, Test Loss: 2.09243, Test Acc: 75.63000\n",
      "F 4/5, Ep 8/20, Loss: 0.01552, Val Acc: 98.98000, Test Loss: 2.38903, Test Acc: 75.33000\n",
      "F 4/5, Ep 9/20, Loss: 0.04019, Val Acc: 98.07999, Test Loss: 2.61085, Test Acc: 74.65000\n",
      "F 4/5, Ep 10/20, Loss: 0.05176, Val Acc: 98.20000, Test Loss: 2.55508, Test Acc: 74.63000\n",
      "F 4/5, Ep 11/20, Loss: 0.03686, Val Acc: 97.57000, Test Loss: 2.65018, Test Acc: 73.70000\n",
      "F 4/5, Ep 12/20, Loss: 0.03782, Val Acc: 97.75000, Test Loss: 2.65536, Test Acc: 73.58000\n",
      "F 4/5, Ep 13/20, Loss: 0.03176, Val Acc: 97.80000, Test Loss: 2.45089, Test Acc: 74.17000\n",
      "F 4/5, Ep 14/20, Loss: 0.03852, Val Acc: 97.82999, Test Loss: 2.42851, Test Acc: 75.21000\n",
      "F 4/5, Ep 15/20, Loss: 0.04355, Val Acc: 97.57999, Test Loss: 2.38237, Test Acc: 74.00000\n",
      "F 4/5, Ep 16/20, Loss: 0.03297, Val Acc: 97.70000, Test Loss: 2.69481, Test Acc: 74.17000\n",
      "F 4/5, Ep 17/20, Loss: 0.03003, Val Acc: 97.80000, Test Loss: 2.69748, Test Acc: 74.87000\n",
      "F 4/5, Ep 18/20, Loss: 0.02396, Val Acc: 97.85000, Test Loss: 2.69589, Test Acc: 75.32000\n",
      "F 4/5, Ep 19/20, Loss: 0.04298, Val Acc: 97.62000, Test Loss: 2.58191, Test Acc: 74.29000\n",
      "F 4/5, Ep 20/20, Loss: 0.03520, Val Acc: 96.85000, Test Loss: 2.65328, Test Acc: 74.08000\n",
      "F 5/5, Ep 1/20, Loss: 0.08721, Val Acc: 99.54000, Test Loss: 2.29910, Test Acc: 74.16000\n",
      "F 5/5, Ep 2/20, Loss: 0.03532, Val Acc: 99.54000, Test Loss: 2.35651, Test Acc: 74.77000\n",
      "F 5/5, Ep 3/20, Loss: 0.02908, Val Acc: 99.17000, Test Loss: 2.29536, Test Acc: 74.77000\n",
      "F 5/5, Ep 4/20, Loss: 0.03089, Val Acc: 99.25000, Test Loss: 2.31760, Test Acc: 74.56000\n",
      "F 5/5, Ep 5/20, Loss: 0.03654, Val Acc: 99.22000, Test Loss: 2.42623, Test Acc: 74.54000\n",
      "F 5/5, Ep 6/20, Loss: 0.03345, Val Acc: 99.23000, Test Loss: 2.37971, Test Acc: 74.74000\n",
      "F 5/5, Ep 7/20, Loss: 0.05819, Val Acc: 98.94000, Test Loss: 2.56979, Test Acc: 74.31000\n",
      "F 5/5, Ep 8/20, Loss: 0.03171, Val Acc: 98.95000, Test Loss: 2.50145, Test Acc: 74.72000\n",
      "F 5/5, Ep 9/20, Loss: 0.02789, Val Acc: 98.83000, Test Loss: 2.51236, Test Acc: 74.40000\n",
      "F 5/5, Ep 10/20, Loss: 0.03256, Val Acc: 99.07000, Test Loss: 2.57693, Test Acc: 75.84000\n",
      "F 5/5, Ep 11/20, Loss: 0.05823, Val Acc: 98.23000, Test Loss: 2.64783, Test Acc: 74.49000\n",
      "F 5/5, Ep 12/20, Loss: 0.03805, Val Acc: 98.94000, Test Loss: 2.63394, Test Acc: 75.20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F 5/5, Ep 13/20, Loss: 0.02000, Val Acc: 98.84000, Test Loss: 2.56310, Test Acc: 75.50000\n",
      "F 5/5, Ep 14/20, Loss: 0.03259, Val Acc: 98.50000, Test Loss: 2.52851, Test Acc: 74.87000\n",
      "F 5/5, Ep 15/20, Loss: 0.03291, Val Acc: 98.68999, Test Loss: 2.78264, Test Acc: 75.16000\n",
      "F 5/5, Ep 16/20, Loss: 0.04295, Val Acc: 96.60000, Test Loss: 2.96683, Test Acc: 73.14000\n",
      "F 5/5, Ep 17/20, Loss: 0.03812, Val Acc: 98.11000, Test Loss: 2.63579, Test Acc: 74.77000\n",
      "F 5/5, Ep 18/20, Loss: 0.03529, Val Acc: 97.89000, Test Loss: 2.86960, Test Acc: 75.17000\n",
      "F 5/5, Ep 19/20, Loss: 0.04429, Val Acc: 97.53000, Test Loss: 2.95977, Test Acc: 74.15000\n",
      "F 5/5, Ep 20/20, Loss: 0.04526, Val Acc: 97.14000, Test Loss: 3.11781, Test Acc: 73.22000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "train = np.array([[x_train[i],y_train[i]] for i in range(len(x_train))])\n",
    "for fold_num, (trn_idx, val_idx) in enumerate(fold.split(train)):\n",
    "    Train_data = train[trn_idx]\n",
    "    Train_data_x = np.array([Train_data[i][0] for i in range(len(Train_data))])\n",
    "    Train_data_y = np.array([Train_data[i][1] for i in range(len(Train_data))])\n",
    "    \n",
    "    Val_data = train[val_idx]\n",
    "    #print(Train_data_x.shape)\n",
    "    Val_data_x = np.array([Val_data[i][0] for i in range(len(Val_data))])\n",
    "    Val_data_y = np.array([Val_data[i][1] for i in range(len(Val_data))])\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((Train_data_x, Train_data_y))\n",
    "    train_ds = train_ds.shuffle(10000).batch(64)\n",
    "    \n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((Val_data_x, Val_data_y))\n",
    "    val_ds = val_ds.shuffle(10000).batch(64)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        for images, labels in train_ds:\n",
    "            train_step(images, labels)\n",
    "\n",
    "        for images, labels in val_ds:\n",
    "            val_step(images, labels)\n",
    "\n",
    "        for test_images, test_labels in test_ds:\n",
    "            test_step(test_images, test_labels)\n",
    "            \n",
    "        print('F {}/{}, Ep {}/{}, Loss: {:.5f}, Val Acc: {:.5f}, Test Loss: {:.5f}, Test Acc: {:.5f}'.\n",
    "            format(\n",
    "                fold_num+1,\n",
    "                5,\n",
    "                epoch+1,\n",
    "                EPOCHS,\n",
    "                train_loss.result(),\n",
    "                val_accuracy.result()*100,\n",
    "                test_loss.result(),\n",
    "                test_accuracy.result()*100))\n",
    "\n",
    "        # Reset the metrics for the next epoch\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        val_loss.reset_states()\n",
    "        val_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
