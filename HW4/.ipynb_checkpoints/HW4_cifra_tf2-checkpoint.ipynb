{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.layers import Dense, \\\n",
    "                                    Flatten, \\\n",
    "                                    Reshape, \\\n",
    "                                    Conv2D, \\\n",
    "                                    MaxPooling2D, \\\n",
    "                                    Dropout,\\\n",
    "                                    ZeroPadding2D,\\\n",
    "                                    BatchNormalization,\\\n",
    "                                    Activation\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "#train, test= tf.keras.datasets.cifar10.load_data()\n",
    "# y_train = np.squeeze(y_train,axis=1)\n",
    "# y_test = np.squeeze(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(length, num):\n",
    "    d = np.zeros(length)\n",
    "    d[num] = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = np.array([onehot(10,i) for i in y_train])\n",
    "# y_test = np.array([onehot(10,i) for i in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (x_train-127.5)/127.5\n",
    "x_test = (x_test-127.5)/127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = np.array([[x_train[i],y_train[i][0]] for i in range(len(x_train))])\n",
    "# for fold_num, (trn_idx, val_idx) in enumerate(fold.split(train)):\n",
    "#     Train_data = train[trn_idx]\n",
    "#     Train_data_x = np.array([Train_data[i][0] for i in range(len(Train_data))])\n",
    "#     Train_data_y = np.array([Train_data[i][1] for i in range(len(Train_data))])\n",
    "    \n",
    "#     Val_data = train[val_idx]\n",
    "#     Val_data_x = np.array([Val_data[i][0] for i in range(len(Val_data))])\n",
    "#     Val_data_y = np.array([Val_data[i][1] for i in range(len(Val_data))])\n",
    "    \n",
    "#     train_ds = tf.data.Dataset.from_tensor_slices((Train_data_x, Train_data_y))\n",
    "#     train_ds = train_ds.shuffle(10000).batch(64)\n",
    "    \n",
    "#     val_ds = tf.data.Dataset.from_tensor_slices((Val_data_x, Val_data_y))\n",
    "#     val_ds = train_ds.shuffle(10000).batch(64)\n",
    "    \n",
    "#     print(fold_num)\n",
    "#     print(Train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(10000).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 32, 32, 3), (None, 1)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = test_ds.shuffle(10000).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdc08c63668>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((x_train[1]*127.5+127.5).astype('int16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Conv2D(64,3, activation='relu',strides = 1,input_shape = (32,32,3)),\n",
    "#     tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=1, padding ='same', data_format='channels_last'),\n",
    "#     tf.keras.layers.Flatten(input_shape=(32,32.3)),##固定輸入\n",
    "#     tf.keras.layers.Dense(128, activation='relu'),\n",
    "#     tf.keras.layers.Dense(10, activation='softmax')    \n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.pad = ZeroPadding2D(2)\n",
    "        \n",
    "        self.conv1 = Conv2D(16, 3,padding=\"same\")\n",
    "        \n",
    "        self.batchn = BatchNormalization()\n",
    "        \n",
    "        self.act1 = Activation(activation = 'relu')\n",
    "        \n",
    "        self.conv2 = Conv2D(160, 3)\n",
    "        \n",
    "        self.act2 = Activation(activation = 'tanh')\n",
    "        \n",
    "        self.conv3 = Conv2D(256, 3)\n",
    "        \n",
    "#         self.conv22 = Conv2D(64, 5,activation='relu',padding=\"valid\")\n",
    "        self.maxpool = MaxPooling2D()\n",
    "        self.drop1  = Dropout(rate = 0.3)\n",
    "#         self.conv2 = Conv2D(64, 3,activation='tanh')\n",
    "#         self.drop2  = Dropout(rate = 0.5)\n",
    "#         self.maxpool2 = MaxPooling2D()\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(2048, activation='relu')\n",
    "        self.d2 = Dense(512, activation='relu')\n",
    "        self.d3 = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x_o): \n",
    "        x = self.pad(x_o)\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchn(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pad(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.batchn(x)\n",
    "        x = self.conv3(x)#32*32*256\n",
    "        x = self.drop1(x)\n",
    "        x = self.maxpool(x)#16*16*256\n",
    "        x = self.act1(x)\n",
    "        \n",
    "        \n",
    "#         #print(\"1: \",x.shape)\n",
    "#         x = self.conv1(x_o)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.drop1(x)\n",
    "        \n",
    "#         y = self.conv22(x_o)# 28*28*32\n",
    "#         y = self.maxpool(y)# 14*14*32\n",
    "#         #print(\"2: \",x.shape)\n",
    "#         x = self.maxpool(x)\n",
    "#         x = self.drop1(x)# 14*14*32\n",
    "#         #print(\"3: \",x.shape)\n",
    "#         x = tf.concat([x, y], 3)\n",
    "        \n",
    "#         x = self.drop2(x)\n",
    "#         x = self.maxpool2(x)\n",
    "#         #print(\"4: \",x.shape)\n",
    "#         x = self.flatten(x)\n",
    "#         #print(\"5: \",x.shape)\n",
    "#         x = self.d1(x)\n",
    "#         x = self.d2(x)\n",
    "#         #print(\"6: \",x.shape)\n",
    "#         x = self.d3(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(logits, labels, gamma):\n",
    "    '''\n",
    "    :param logits:  [batch_size, n_class]\n",
    "    :param labels: [batch_size]\n",
    "    :return: -(1-y)^r * log(y)\n",
    "    '''\n",
    "    labels = tf.cast(labels, tf.int32)\n",
    "    softmax = tf.reshape(tf.nn.softmax(logits), [-1])  # [batch_size * n_class]\n",
    "    labels = tf.range(0, logits.shape[0]) * logits.shape[1] + labels\n",
    "    prob = tf.gather(softmax, labels)\n",
    "    weight = tf.pow(tf.subtract(1., prob), gamma)\n",
    "    loss = -tf.reduce_mean(tf.multiply(weight, tf.log(prob)))\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_category_focal_loss1(y_true, y_pred):\n",
    "    epsilon = 1.e-7\n",
    "    gamma = 2.0\n",
    "    #alpha = tf.constant([[2],[1],[1],[1],[1]], dtype=tf.float32)\n",
    "    alpha = tf.constant([[1],[1],[1],[1],[1],[1],[1],[1],[1],[1]], dtype=tf.float32)\n",
    "\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "    y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "    ce = -tf.math.log(y_t/10)\n",
    "    weight = tf.pow(tf.subtract(1., y_t), gamma)\n",
    "    #print(weight)\n",
    "    fl = tf.matmul(tf.multiply(weight, ce), alpha)\n",
    "    loss = tf.reduce_mean(fl)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, labels):\n",
    "    #print(\"I :\",images.shape)\n",
    "    #print(\"L :\",labels)\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        #print(\"P: \",predictions.shape)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        #print(loss)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    #print(\"train loss: \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    loss = loss_object(labels, predictions)\n",
    "    val_loss(loss)\n",
    "    val_accuracy(labels, predictions)\n",
    "    #print(\"val loss: \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 20\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     for images, labels in train_ds:\n",
    "#         train_step(images, labels)\n",
    "\n",
    "#     for test_images, test_labels in test_ds:\n",
    "#         test_step(test_images, test_labels)\n",
    "\n",
    "#     #template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "#     print('Epoch {}/{}, Loss: {:.5f}, Accuracy: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.\n",
    "#             format(\n",
    "#                 epoch+1,\n",
    "#                 EPOCHS,\n",
    "#                 train_loss.result(),\n",
    "#                 train_accuracy.result()*100,\n",
    "#                 test_loss.result(),\n",
    "#                 test_accuracy.result()*100))\n",
    "\n",
    "#     # Reset the metrics for the next epoch\n",
    "#     train_loss.reset_states()\n",
    "#     train_accuracy.reset_states()\n",
    "#     test_loss.reset_states()\n",
    "#     test_accuracy.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = KFold(n_splits = 5, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F 1/5, Ep 1/20, Loss: 1.28018, Val Acc: 66.97000, Test Loss: 0.94791, Test Acc: 66.98000\n",
      "F 1/5, Ep 2/20, Loss: 0.81581, Val Acc: 71.32000, Test Loss: 0.82028, Test Acc: 71.03001\n",
      "F 1/5, Ep 3/20, Loss: 0.55958, Val Acc: 74.11000, Test Loss: 0.81661, Test Acc: 73.03000\n",
      "F 1/5, Ep 4/20, Loss: 0.33353, Val Acc: 74.07000, Test Loss: 0.86996, Test Acc: 73.32000\n",
      "F 1/5, Ep 5/20, Loss: 0.17276, Val Acc: 74.24000, Test Loss: 1.13422, Test Acc: 73.53000\n",
      "F 1/5, Ep 6/20, Loss: 0.12097, Val Acc: 74.83000, Test Loss: 1.22659, Test Acc: 73.65000\n",
      "F 1/5, Ep 7/20, Loss: 0.09192, Val Acc: 74.31000, Test Loss: 1.30473, Test Acc: 73.74000\n",
      "F 1/5, Ep 8/20, Loss: 0.08449, Val Acc: 74.75000, Test Loss: 1.37619, Test Acc: 73.28000\n",
      "F 1/5, Ep 9/20, Loss: 0.07003, Val Acc: 74.23000, Test Loss: 1.45415, Test Acc: 73.50999\n",
      "F 1/5, Ep 10/20, Loss: 0.07435, Val Acc: 73.30000, Test Loss: 1.50365, Test Acc: 73.00999\n",
      "F 1/5, Ep 11/20, Loss: 0.05891, Val Acc: 74.73000, Test Loss: 1.58918, Test Acc: 73.94000\n",
      "F 1/5, Ep 12/20, Loss: 0.06250, Val Acc: 73.07000, Test Loss: 1.74179, Test Acc: 72.55000\n",
      "F 1/5, Ep 13/20, Loss: 0.05613, Val Acc: 75.20000, Test Loss: 1.69225, Test Acc: 74.05000\n",
      "F 1/5, Ep 14/20, Loss: 0.05140, Val Acc: 73.83000, Test Loss: 1.76405, Test Acc: 72.75999\n",
      "F 1/5, Ep 15/20, Loss: 0.05163, Val Acc: 73.94000, Test Loss: 1.70964, Test Acc: 73.45000\n",
      "F 1/5, Ep 16/20, Loss: 0.05395, Val Acc: 73.89000, Test Loss: 1.75462, Test Acc: 73.68999\n",
      "F 1/5, Ep 17/20, Loss: 0.04309, Val Acc: 74.11000, Test Loss: 1.80080, Test Acc: 74.09000\n",
      "F 1/5, Ep 18/20, Loss: 0.04054, Val Acc: 74.03000, Test Loss: 1.97131, Test Acc: 73.63000\n",
      "F 1/5, Ep 19/20, Loss: 0.04880, Val Acc: 74.09000, Test Loss: 1.83006, Test Acc: 73.30000\n",
      "F 1/5, Ep 20/20, Loss: 0.04512, Val Acc: 74.30000, Test Loss: 1.99418, Test Acc: 73.83000\n",
      "F 2/5, Ep 1/20, Loss: 0.30810, Val Acc: 99.00000, Test Loss: 1.01621, Test Acc: 75.58000\n",
      "F 2/5, Ep 2/20, Loss: 0.10349, Val Acc: 98.40000, Test Loss: 1.20328, Test Acc: 76.16000\n",
      "F 2/5, Ep 3/20, Loss: 0.03395, Val Acc: 97.07000, Test Loss: 1.47798, Test Acc: 74.23000\n",
      "F 2/5, Ep 4/20, Loss: 0.03250, Val Acc: 97.39000, Test Loss: 1.62359, Test Acc: 74.09000\n",
      "F 2/5, Ep 5/20, Loss: 0.05719, Val Acc: 96.38000, Test Loss: 1.64868, Test Acc: 73.85000\n",
      "F 2/5, Ep 6/20, Loss: 0.05359, Val Acc: 96.74000, Test Loss: 1.69361, Test Acc: 74.93000\n",
      "F 2/5, Ep 7/20, Loss: 0.04487, Val Acc: 94.50000, Test Loss: 1.78892, Test Acc: 72.66000\n",
      "F 2/5, Ep 8/20, Loss: 0.04878, Val Acc: 95.86000, Test Loss: 1.68204, Test Acc: 73.93000\n",
      "F 2/5, Ep 9/20, Loss: 0.02882, Val Acc: 96.32000, Test Loss: 1.74692, Test Acc: 74.84000\n",
      "F 2/5, Ep 10/20, Loss: 0.04125, Val Acc: 94.87000, Test Loss: 1.85465, Test Acc: 74.03000\n",
      "F 2/5, Ep 11/20, Loss: 0.04641, Val Acc: 94.73000, Test Loss: 1.82711, Test Acc: 73.61000\n",
      "F 2/5, Ep 12/20, Loss: 0.03646, Val Acc: 94.80000, Test Loss: 2.02743, Test Acc: 74.19000\n",
      "F 2/5, Ep 13/20, Loss: 0.04658, Val Acc: 93.61000, Test Loss: 2.03157, Test Acc: 74.05000\n",
      "F 2/5, Ep 14/20, Loss: 0.03819, Val Acc: 94.22000, Test Loss: 1.98018, Test Acc: 74.04000\n",
      "F 2/5, Ep 15/20, Loss: 0.04204, Val Acc: 93.72000, Test Loss: 1.95273, Test Acc: 74.81000\n",
      "F 2/5, Ep 16/20, Loss: 0.03307, Val Acc: 91.43000, Test Loss: 2.11379, Test Acc: 72.98000\n",
      "F 2/5, Ep 17/20, Loss: 0.03501, Val Acc: 92.84000, Test Loss: 2.07219, Test Acc: 74.29000\n",
      "F 2/5, Ep 18/20, Loss: 0.03320, Val Acc: 92.48000, Test Loss: 2.03408, Test Acc: 74.51000\n",
      "F 2/5, Ep 19/20, Loss: 0.03696, Val Acc: 91.45000, Test Loss: 1.90881, Test Acc: 73.38000\n",
      "F 2/5, Ep 20/20, Loss: 0.03740, Val Acc: 91.66000, Test Loss: 2.05713, Test Acc: 74.89000\n",
      "F 3/5, Ep 1/20, Loss: 0.12479, Val Acc: 99.32000, Test Loss: 1.60368, Test Acc: 75.09000\n",
      "F 3/5, Ep 2/20, Loss: 0.03357, Val Acc: 99.31000, Test Loss: 1.76918, Test Acc: 75.82000\n",
      "F 3/5, Ep 3/20, Loss: 0.01450, Val Acc: 99.10000, Test Loss: 1.96313, Test Acc: 75.24000\n",
      "F 3/5, Ep 4/20, Loss: 0.04001, Val Acc: 98.15000, Test Loss: 2.15683, Test Acc: 74.27000\n",
      "F 3/5, Ep 5/20, Loss: 0.04028, Val Acc: 98.43999, Test Loss: 1.98771, Test Acc: 74.50000\n",
      "F 3/5, Ep 6/20, Loss: 0.03876, Val Acc: 97.93000, Test Loss: 2.14198, Test Acc: 73.91000\n",
      "F 3/5, Ep 7/20, Loss: 0.03930, Val Acc: 97.59000, Test Loss: 2.05282, Test Acc: 73.59000\n",
      "F 3/5, Ep 8/20, Loss: 0.04118, Val Acc: 98.18000, Test Loss: 2.04433, Test Acc: 74.59000\n",
      "F 3/5, Ep 9/20, Loss: 0.03568, Val Acc: 98.00000, Test Loss: 2.01719, Test Acc: 74.90000\n",
      "F 3/5, Ep 10/20, Loss: 0.03680, Val Acc: 98.16000, Test Loss: 2.17651, Test Acc: 74.90000\n",
      "F 3/5, Ep 11/20, Loss: 0.03036, Val Acc: 97.43999, Test Loss: 2.36731, Test Acc: 74.87000\n",
      "F 3/5, Ep 12/20, Loss: 0.03509, Val Acc: 96.34000, Test Loss: 2.24870, Test Acc: 73.60000\n",
      "F 3/5, Ep 13/20, Loss: 0.04929, Val Acc: 97.46000, Test Loss: 2.21599, Test Acc: 74.35000\n",
      "F 3/5, Ep 14/20, Loss: 0.02853, Val Acc: 97.35000, Test Loss: 2.21917, Test Acc: 74.20000\n",
      "F 3/5, Ep 15/20, Loss: 0.04293, Val Acc: 96.83000, Test Loss: 2.43097, Test Acc: 74.53000\n",
      "F 3/5, Ep 16/20, Loss: 0.04091, Val Acc: 96.34000, Test Loss: 2.34428, Test Acc: 73.97000\n",
      "F 3/5, Ep 17/20, Loss: 0.03300, Val Acc: 95.59000, Test Loss: 2.41884, Test Acc: 73.36000\n",
      "F 3/5, Ep 18/20, Loss: 0.03817, Val Acc: 96.68000, Test Loss: 2.29478, Test Acc: 74.38000\n",
      "F 3/5, Ep 19/20, Loss: 0.03110, Val Acc: 96.88000, Test Loss: 2.38923, Test Acc: 75.10000\n",
      "F 3/5, Ep 20/20, Loss: 0.04061, Val Acc: 94.44000, Test Loss: 2.66778, Test Acc: 73.24000\n",
      "F 4/5, Ep 1/20, Loss: 0.08836, Val Acc: 99.38000, Test Loss: 1.98028, Test Acc: 74.16000\n",
      "F 4/5, Ep 2/20, Loss: 0.03242, Val Acc: 99.38000, Test Loss: 1.99705, Test Acc: 74.64000\n",
      "F 4/5, Ep 3/20, Loss: 0.02165, Val Acc: 99.38000, Test Loss: 2.03022, Test Acc: 75.01000\n",
      "F 4/5, Ep 4/20, Loss: 0.02636, Val Acc: 98.84000, Test Loss: 2.27948, Test Acc: 74.10000\n",
      "F 4/5, Ep 5/20, Loss: 0.04130, Val Acc: 98.77000, Test Loss: 2.41661, Test Acc: 74.73000\n",
      "F 4/5, Ep 6/20, Loss: 0.05101, Val Acc: 98.83000, Test Loss: 2.22133, Test Acc: 75.10000\n",
      "F 4/5, Ep 7/20, Loss: 0.03207, Val Acc: 99.01000, Test Loss: 2.09243, Test Acc: 75.63000\n",
      "F 4/5, Ep 8/20, Loss: 0.01552, Val Acc: 98.98000, Test Loss: 2.38903, Test Acc: 75.33000\n",
      "F 4/5, Ep 9/20, Loss: 0.04019, Val Acc: 98.07999, Test Loss: 2.61085, Test Acc: 74.65000\n",
      "F 4/5, Ep 10/20, Loss: 0.05176, Val Acc: 98.20000, Test Loss: 2.55508, Test Acc: 74.63000\n",
      "F 4/5, Ep 11/20, Loss: 0.03686, Val Acc: 97.57000, Test Loss: 2.65018, Test Acc: 73.70000\n",
      "F 4/5, Ep 12/20, Loss: 0.03782, Val Acc: 97.75000, Test Loss: 2.65536, Test Acc: 73.58000\n",
      "F 4/5, Ep 13/20, Loss: 0.03176, Val Acc: 97.80000, Test Loss: 2.45089, Test Acc: 74.17000\n",
      "F 4/5, Ep 14/20, Loss: 0.03852, Val Acc: 97.82999, Test Loss: 2.42851, Test Acc: 75.21000\n",
      "F 4/5, Ep 15/20, Loss: 0.04355, Val Acc: 97.57999, Test Loss: 2.38237, Test Acc: 74.00000\n",
      "F 4/5, Ep 16/20, Loss: 0.03297, Val Acc: 97.70000, Test Loss: 2.69481, Test Acc: 74.17000\n",
      "F 4/5, Ep 17/20, Loss: 0.03003, Val Acc: 97.80000, Test Loss: 2.69748, Test Acc: 74.87000\n",
      "F 4/5, Ep 18/20, Loss: 0.02396, Val Acc: 97.85000, Test Loss: 2.69589, Test Acc: 75.32000\n",
      "F 4/5, Ep 19/20, Loss: 0.04298, Val Acc: 97.62000, Test Loss: 2.58191, Test Acc: 74.29000\n",
      "F 4/5, Ep 20/20, Loss: 0.03520, Val Acc: 96.85000, Test Loss: 2.65328, Test Acc: 74.08000\n",
      "F 5/5, Ep 1/20, Loss: 0.08721, Val Acc: 99.54000, Test Loss: 2.29910, Test Acc: 74.16000\n",
      "F 5/5, Ep 2/20, Loss: 0.03532, Val Acc: 99.54000, Test Loss: 2.35651, Test Acc: 74.77000\n",
      "F 5/5, Ep 3/20, Loss: 0.02908, Val Acc: 99.17000, Test Loss: 2.29536, Test Acc: 74.77000\n",
      "F 5/5, Ep 4/20, Loss: 0.03089, Val Acc: 99.25000, Test Loss: 2.31760, Test Acc: 74.56000\n",
      "F 5/5, Ep 5/20, Loss: 0.03654, Val Acc: 99.22000, Test Loss: 2.42623, Test Acc: 74.54000\n",
      "F 5/5, Ep 6/20, Loss: 0.03345, Val Acc: 99.23000, Test Loss: 2.37971, Test Acc: 74.74000\n",
      "F 5/5, Ep 7/20, Loss: 0.05819, Val Acc: 98.94000, Test Loss: 2.56979, Test Acc: 74.31000\n",
      "F 5/5, Ep 8/20, Loss: 0.03171, Val Acc: 98.95000, Test Loss: 2.50145, Test Acc: 74.72000\n",
      "F 5/5, Ep 9/20, Loss: 0.02789, Val Acc: 98.83000, Test Loss: 2.51236, Test Acc: 74.40000\n",
      "F 5/5, Ep 10/20, Loss: 0.03256, Val Acc: 99.07000, Test Loss: 2.57693, Test Acc: 75.84000\n",
      "F 5/5, Ep 11/20, Loss: 0.05823, Val Acc: 98.23000, Test Loss: 2.64783, Test Acc: 74.49000\n",
      "F 5/5, Ep 12/20, Loss: 0.03805, Val Acc: 98.94000, Test Loss: 2.63394, Test Acc: 75.20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F 5/5, Ep 13/20, Loss: 0.02000, Val Acc: 98.84000, Test Loss: 2.56310, Test Acc: 75.50000\n",
      "F 5/5, Ep 14/20, Loss: 0.03259, Val Acc: 98.50000, Test Loss: 2.52851, Test Acc: 74.87000\n",
      "F 5/5, Ep 15/20, Loss: 0.03291, Val Acc: 98.68999, Test Loss: 2.78264, Test Acc: 75.16000\n",
      "F 5/5, Ep 16/20, Loss: 0.04295, Val Acc: 96.60000, Test Loss: 2.96683, Test Acc: 73.14000\n",
      "F 5/5, Ep 17/20, Loss: 0.03812, Val Acc: 98.11000, Test Loss: 2.63579, Test Acc: 74.77000\n",
      "F 5/5, Ep 18/20, Loss: 0.03529, Val Acc: 97.89000, Test Loss: 2.86960, Test Acc: 75.17000\n",
      "F 5/5, Ep 19/20, Loss: 0.04429, Val Acc: 97.53000, Test Loss: 2.95977, Test Acc: 74.15000\n",
      "F 5/5, Ep 20/20, Loss: 0.04526, Val Acc: 97.14000, Test Loss: 3.11781, Test Acc: 73.22000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "train = np.array([[x_train[i],y_train[i]] for i in range(len(x_train))])\n",
    "for fold_num, (trn_idx, val_idx) in enumerate(fold.split(train)):\n",
    "    Train_data = train[trn_idx]\n",
    "    Train_data_x = np.array([Train_data[i][0] for i in range(len(Train_data))])\n",
    "    Train_data_y = np.array([Train_data[i][1] for i in range(len(Train_data))])\n",
    "    \n",
    "    Val_data = train[val_idx]\n",
    "    #print(Train_data_x.shape)\n",
    "    Val_data_x = np.array([Val_data[i][0] for i in range(len(Val_data))])\n",
    "    Val_data_y = np.array([Val_data[i][1] for i in range(len(Val_data))])\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((Train_data_x, Train_data_y))\n",
    "    train_ds = train_ds.shuffle(10000).batch(64)\n",
    "    \n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((Val_data_x, Val_data_y))\n",
    "    val_ds = val_ds.shuffle(10000).batch(64)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        for images, labels in train_ds:\n",
    "            train_step(images, labels)\n",
    "\n",
    "        for images, labels in val_ds:\n",
    "            val_step(images, labels)\n",
    "\n",
    "        for test_images, test_labels in test_ds:\n",
    "            test_step(test_images, test_labels)\n",
    "            \n",
    "        print('F {}/{}, Ep {}/{}, Loss: {:.5f}, Val Acc: {:.5f}, Test Loss: {:.5f}, Test Acc: {:.5f}'.\n",
    "            format(\n",
    "                fold_num+1,\n",
    "                5,\n",
    "                epoch+1,\n",
    "                EPOCHS,\n",
    "                train_loss.result(),\n",
    "                val_accuracy.result()*100,\n",
    "                test_loss.result(),\n",
    "                test_accuracy.result()*100))\n",
    "\n",
    "        # Reset the metrics for the next epoch\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        val_loss.reset_states()\n",
    "        val_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
