{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      " \"cells\": [\r\n",
      "  {\r\n",
      "   \"cell_type\": \"code\",\r\n",
      "   \"execution_count\": 1,\r\n",
      "   \"metadata\": {\r\n",
      "    \"collapsed\": true\r\n",
      "   },\r\n",
      "   \"outputs\": [],\r\n",
      "   \"source\": [\r\n",
      "    \"import numpy as np\\n\",\r\n",
      "    \"\\n\",\r\n",
      "    \"def Matrix(*a):\\n\",\r\n",
      "    \"    if len(a)==1 and isinstance(a[0], np.ndarray):\\n\",\r\n",
      "    \"        a = a[0]\\n\",\r\n",
      "    \"    return np.array([[float(x) for x in r] for r in a])\\n\",\r\n",
      "    \"\\n\",\r\n",
      "    \"def Vector(*a):\\n\",\r\n",
      "    \"    if len(a)==1 and isinstance(a[0], np.ndarray):\\n\",\r\n",
      "    \"        a = a[0]\\n\",\r\n",
      "    \"    return np.array([float(x) for x in a]).reshape(-1,1)\\n\",\r\n",
      "    \"\\n\",\r\n",
      "    \"# Black magic\\n\",\r\n",
      "    \"from IPython.display import Latex, SVG, display\\n\",\r\n",
      "    \"from IPython.core.interactiveshell import InteractiveShell\\n\",\r\n",
      "    \"\\n\",\r\n",
      "    \"def ndarray_to_latex(arr): \\n\",\r\n",
      "    \"    if len(arr.shape)==1: \\n\",\r\n",
      "    \"        arr=arr.reshape(1,-1)\\n\",\r\n",
      "    \"    if len(arr.shape) == 2:\\n\",\r\n",
      "    \"        if max(arr.shape) > 30:\\n\",\r\n",
      "    \"            return None\\n\",\r\n",
      "    \"        str_arr = np.vectorize(\\\"{:.3f}\\\".format)(arr)\\n\",\r\n",
      "    \"        return r'\\\\begin{{pmatrix}}{}\\\\end{{pmatrix}}'.format(r'\\\\\\\\ '.join(map('&'.join, str_arr))) \\n\",\r\n",
      "    \"    if len(arr.shape) == 3 and arr.shape[2]==1:\\n\",\r\n",
      "    \"        if max(arr.shape) > 30:\\n\",\r\n",
      "    \"            return None\\n\",\r\n",
      "    \"        arr = arr[:,:,0]\\n\",\r\n",
      "    \"        str_arr = np.vectorize(\\\"{:.3f}\\\".format)(arr)\\n\",\r\n",
      "    \"        return r'\\\\begin{{bmatrix}}[{}]\\\\end{{bmatrix}}'.format(\\n\",\r\n",
      "    \"            r']\\\\\\\\ ['.join(map('&'.join, str_arr))) \\n\",\r\n",
      "    \"    return None\\n\",\r\n",
      "    \"sh = InteractiveShell.instance()\\n\",\r\n",
      "    \"sh.display_formatter.formatters['text/latex'].type_printers[np.ndarray]=ndarray_to_latex\"\r\n",
      "   ]\r\n",
      "  },\r\n",
      "  {\r\n",
      "   \"cell_type\": \"code\",\r\n",
      "   \"execution_count\": null,\r\n",
      "   \"metadata\": {\r\n",
      "    \"collapsed\": true\r\n",
      "   },\r\n",
      "   \"outputs\": [],\r\n",
      "   \"source\": []\r\n",
      "  }\r\n",
      " ],\r\n",
      " \"metadata\": {\r\n",
      "  \"kernelspec\": {\r\n",
      "   \"display_name\": \"Python 3\",\r\n",
      "   \"language\": \"python\",\r\n",
      "   \"name\": \"python3\"\r\n",
      "  },\r\n",
      "  \"language_info\": {\r\n",
      "   \"codemirror_mode\": {\r\n",
      "    \"name\": \"ipython\",\r\n",
      "    \"version\": 3\r\n",
      "   },\r\n",
      "   \"file_extension\": \".py\",\r\n",
      "   \"mimetype\": \"text/x-python\",\r\n",
      "   \"name\": \"python\",\r\n",
      "   \"nbconvert_exporter\": \"python\",\r\n",
      "   \"pygments_lexer\": \"ipython3\",\r\n",
      "   \"version\": \"3.6.5\"\r\n",
      "  }\r\n",
      " },\r\n",
      " \"nbformat\": 4,\r\n",
      " \"nbformat_minor\": 2\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! cat magic.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run magic.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning for classification\n",
    "給一堆 $x$, 和他的分類，我們找出計算 x 的分類的方式\n",
    "\n",
    "### One hot encoding\n",
    "如果我們有三類種類別， 我們可以來編碼這三個類別\n",
    "* $(1,0,0)$\n",
    "* $(0,1,0)$\n",
    "* $(0,0,1)$\n",
    "\n",
    "### 問題\n",
    "* 為什麼不直接用 1,2,3 這樣的編碼呢？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression 的模型是這樣的\n",
    "我們的輸入 $x=\\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} $ 是一個向量，我們看成 column vector 好了\n",
    "\n",
    "而 Weight: $W = \\begin{pmatrix} W_0 \\\\ W_1 \\\\ W_2 \\end{pmatrix} =  \n",
    "\\begin{pmatrix} W_{0,0} & W_{0,1} &  W_{0,2} & W_{0,3}\\\\ \n",
    " W_{1,0} & W_{1,1} &  W_{1,2} & W_{1,3} \\\\ \n",
    " W_{2,0} & W_{2,1} &  W_{2,2} & W_{2,3} \\end{pmatrix} $\n",
    " \n",
    " Bias: $b=\\begin{pmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{pmatrix} $ \n",
    "\n",
    "\n",
    "我們先計算\"線性輸出\"  $ c = \\begin{pmatrix} c_0 \\\\ c_1 \\\\ c_2 \\end{pmatrix} =  Wx+b =\n",
    "\\begin{pmatrix} W_0 x + b_0 \\\\ W_1 x + b_1 \\\\ W_2 x + b_2 \\end{pmatrix}   $， 然後再取 $exp$ (逐項取)。 最後得到一個向量。\n",
    " \n",
    " $d = \\begin{pmatrix} d_0 \\\\ d_1 \\\\ d_2 \\end{pmatrix} = e^{W x + b} = \\begin{pmatrix} e^{c_0} \\\\ e^{c_1} \\\\ e^{c_2} \\end{pmatrix}$\n",
    "\n",
    "\n",
    "將這些數值除以他們的總和。\n",
    "給定輸入 x， 我們希望算出來的數字 q_i 會符合 x 的類別是 i 的機率。\n",
    "\n",
    "###  $q_i = Predict_{W,b}(Y=i|x)  = \\frac {e^{W_i x + b_i}} {\\sum_j e^{W_j x + b_j}} = \\frac {d_i} {\\sum_j d_j}$\n",
    "\n",
    "### 合起來看，就是 $q = \\frac {d} {\\sum_j d_j} $\n",
    "\n",
    "### 問題\n",
    "* 為什麼要用 $exp$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先隨便算一個 $\\mathbb{R}^2 \\rightarrow \\mathbb{R}^3$ 的網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.000&2.000\\\\ 3.000&4.000\\\\ 5.000&6.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.],\n",
       "       [5., 6.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weight\n",
    "W = Matrix([1,2],[3,4], [5,6])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.000\\\\ 0.000\\\\ -1.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bias\n",
    "b = Vector(1,0,-1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}2.000\\\\ -1.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 2.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 輸入\n",
    "x = Vector(2,-1)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任務：計算最後的猜測機率 $q$\n",
    "Hint: `np.exp` 可以算 $exp$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.71828183   7.3890561 ]\n",
      " [ 20.08553692  54.59815003]\n",
      " [148.4131591  403.42879349]]\n",
      "[[2.71828183]\n",
      " [1.        ]\n",
      " [0.36787944]]\n",
      "[[7.3890561 ]\n",
      " [0.36787944]]\n"
     ]
    }
   ],
   "source": [
    "# 請在這裡計算\n",
    "print(np.exp(W))\n",
    "print(np.exp(b))\n",
    "print(np.exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Wx+b\r\n",
      "c = W @ x + b\r\n",
      "\r\n",
      "# d = exp(Wx+b)\r\n",
      "d = np.exp(c)\r\n",
      "\r\n",
      "# q = d/sum(d)\r\n",
      "q = d/d.sum()\r\n"
     ]
    }
   ],
   "source": [
    "! cat solutions/softmax_compute_q.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參考答案\n",
    "#%load solutions/softmax_compute_q.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Wx+b\r\n",
      "c = W @ x + b\r\n",
      "\r\n",
      "# d = exp(Wx+b)\r\n",
      "d = np.exp(c)\r\n",
      "\r\n",
      "# q = d/sum(d)\r\n",
      "q = d/d.sum()\r\n"
     ]
    }
   ],
   "source": [
    "! cat solutions/softmax_compute_q.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.090\\\\ 0.245\\\\ 0.665\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[0.09003057],\n",
       "       [0.24472847],\n",
       "       [0.66524096]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run -i solutions/softmax_compute_q.py\n",
    "# 顯示 q\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習\n",
    "設計一個網路:\n",
    "* 輸入是二進位 0 ~ 15\n",
    "* 輸出依照對於 4 的餘數分成四類\n",
    "\n",
    "Hint: 可以參考上面 W, b 的設定方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.000\\\\ 0.000\\\\ 1.000\\\\ 1.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hint 下面產生數字 i 的 2 進位向量\n",
    "i = 13 # 1011 in bin\n",
    "x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 請在這裡計算\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = Matrix([-1,-1,0,0], [1,-1,0,0], [-1,1,0,0], [1,1,0,0])\r\n",
      "b = Vector(0,0,0,0)\r\n",
      "for i in range(16):\r\n",
      "    x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\r\n",
      "    r = W @ x + b\r\n",
      "    print(\"i=\", i, \"predict:\", r.argmax(), \"ground truth:\", i%4)\r\n"
     ]
    }
   ],
   "source": [
    "! cat solutions/softmax_mod4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參考答案\n",
    "#%load solutions/softmax_mod4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習\n",
    "設計一個網路:\n",
    "* 輸入是二進位 0 ~ 15\n",
    "* 輸出依照對於 3 的餘數分成三類\n",
    "\n",
    "Hint: 不用全部正確，用猜的，但正確率要比亂猜高。可以利用統計的結果猜猜看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 請在這裡計算\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參考答案\n",
    "#%load solutions/softmax_mod3.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差函數\n",
    "為了要評斷我們的預測的品質，要設計一個評斷誤差的方式\n",
    "\n",
    "假設輸入值 $x$ 對應到的真實類別是 $y$, 那我們定義誤差函數\n",
    "\n",
    "## $ loss = -\\log(q_y)=- \\log(Predict_{W,b}(Y=y|x)) $\n",
    "\n",
    "這個方法叫做 Cross entropy\n",
    "\n",
    "其實比較一般但比較複雜一點的寫法是\n",
    "\n",
    "## $ loss = - \\sum_i p_i\\log(q_i)  = -  p \\cdot \\log q$\n",
    "\n",
    "其中 $i$ 是所有類別， 而 $ p_i = \\Pr(Y=i|x) $ 是真實發生的機率\n",
    "\n",
    "但我們目前 $x$ 對應到的真實類別是 $y$， 所以直接 $p_i = 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 想辦法改進。 \n",
    "我們用一種被稱作是 gradient descent 的方式來改善我們的誤差。\n",
    "\n",
    "因為我們知道 gradient 是讓函數上升最快的方向。所以我們如果朝 gradient 的反方向走一點點（也就是下降最快的方向），那麼得到的函數值應該會小一點。\n",
    "\n",
    "記得我們的變數是 $W$ 和 $b$ (裡面有一堆 W_i,j b_i 這些變數)，所以我們要把 $loss$ 對 $W$ 和 $b$ 裡面的每一個參數來偏微分。\n",
    "\n",
    "還好這個偏微分是可以用手算出他的形式，而最後偏微分的式子也不會很複雜。\n",
    "\n",
    "$loss$ 展開後可以寫成\n",
    "## $loss = -\\log(q_y) = \\log(\\sum_j d_j) - d_i \\\\\n",
    " = \\log(\\sum_j e^{W_j x + b_j}) - W_i x - b_i$\n",
    "\n",
    "注意 $d_j = e^{W_j x + b_j}$ 只有變數 $b_j, W_j$ \n",
    "\n",
    " 對 $k \\neq i$ 時, $loss$ 對 $b_k$ 的偏微分是 \n",
    " $$ \\frac{e^{W_k x + b_k}}{\\sum_j e^{W_j x + b_j}} = q_k$$\n",
    "對 $k = i$ 時, $loss$ 對 $b_k$ 的偏微分是 \n",
    "$$ q_k - 1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對 $W$ 的偏微分也不難\n",
    "\n",
    " 對 $k \\neq i$ 時, $loss$ 對 $W_{k,t}$ 的偏微分是 \n",
    " $$ \\frac{e^{W_k x + b_k}  x_t}{\\sum_j e^{W_j x + b_j}} = q_k x_t$$\n",
    "對 $k = i$ 時, $loss$ 對 $W_{k,t}$ 的偏微分是 \n",
    "$$ q_k x_t - x_t$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實做部份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 先產生隨機的 W 和 b\n",
    "W = Matrix(np.random.normal(size=(3,4)))\n",
    "b = Vector(np.random.normal(size=(3,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題\n",
    "W, b 的 size 為什麼要這樣設定？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任務： 隨便設定一組 x, y, 我們來跑跑看 gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 14\n",
    "x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\n",
    "y = i%3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟：計算 q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 請在這裡計算\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參考答案(跟前面一樣)¶ \n",
    "#%load solutions/softmax_compute_q.py\n",
    "%run -i solutions/softmax_compute_q.py\n",
    "#顯示 q\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟： 計算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 請在這裡計算 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參考答案(跟前面一樣)\n",
    "%run -i solutions/softmax_compute_loss1.py\n",
    "#顯示 loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟：計算對 b 的 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 請在這裡計算 grad_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#參考答案\n",
    "%run -i  solutions/softmax_compute_grad_b.py\n",
    "grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟：計算對 W 的 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 請在這裡計算\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#參考答案\n",
    "%run -i  solutions/softmax_compute_grad_W.py\n",
    "grad_W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟：更新 W, b  各減掉 0.5 * gradient， 然後看看新的 loss 是否有進步了？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 請在這裡計算\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參考答案\n",
    "%run -i solutions/softmax_update_Wb.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原先的 q\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原先的 loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 現在的 loss\n",
    "%run -i solutions/softmax_compute_q.py\n",
    "%run -i solutions/softmax_compute_loss1.py\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一次訓練多組資料\n",
    "上面只針對一組 x (i=14) 來訓練，如果一次對所有 x 訓練呢？\n",
    "\n",
    "通常我們會把組別放在 axis-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2) for i in range(16)])\n",
    "for i in range(4):\n",
    "    print(\"i=\", i)\n",
    "    display(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對應的組別 \n",
    "y = np.array([i%3 for i in range(16)])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任務： 將訓練向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 請在這裡計算\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 參考解答如後"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對照\n",
    "```python\n",
    "d = np.exp(W @ x + b)\n",
    "q = d/d.sum()\n",
    "q\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.exp(W @ X + b)\n",
    "q = d/d.sum(axis=(1,2), keepdims=True)\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對照\n",
    "```python\n",
    "loss = -np.log(q[y])\n",
    "loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -np.log(q[range(len(y)), y])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用平均當成我們真正的 loss\n",
    "loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對照\n",
    "```python\n",
    "grad_b = q - np.eye(3)[y][:, None]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy indexing :p\n",
    "one_y = np.eye(3)[y][..., None]\n",
    "grad_b_all = q - one_y\n",
    "grad_b = grad_b_all.mean(axis=0)\n",
    "grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對照\n",
    "```python\n",
    "grad_W = grad_b @ x.T\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_W_all = grad_b_all @ X.swapaxes(1,2)\n",
    "grad_W = grad_W_all.mean(axis=0)\n",
    "grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W -=  0.5 * grad_W\n",
    "b -=  0.5 * grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 之前的 loss\n",
    "loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.exp(W @ X + b)\n",
    "q = d/d.sum(axis=(1,2), keepdims=True)\n",
    "loss = -np.log(q[range(len(y)), y])\n",
    "loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 任務：全部合在一起\n",
    "* 設定 W,b\n",
    "* 設定 X\n",
    "* 訓練三十次\n",
    "    * 計算 q 和 loss    \n",
    "    * 計算 grad_b 和 grad_W\n",
    "    * 更新 W, b\n",
    "* 看看準確度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 在這裡計算\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參考答案\n",
    "%run -i solutions/softmax_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 畫出 loss 的曲線\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對答案\n",
    "display((W @ X + b).argmax(axis=1).ravel())\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
